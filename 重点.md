# 第一章

滤波器怎么计算，怎么用，为什么要用图像滤波

**滤波**：对图像中每个像素的局部邻域计算一个函数，函数由“滤波器”或掩码指定，说明如何组合邻域中的值。

## 滤波的用途:

增强图像（去噪、调整大小等）

提取信息（纹理、边缘等）

检测模式（模板匹配）

## 线性滤波器

盒式滤波器：所有像素取平均值，会模糊边缘

高斯滤波器：中间权重高，四周低。平滑效果更自然

## 非线性滤波器

中值滤波器：取窗口内像素的中位数，中值滤波器的最大优点是边缘保留，并且去除“椒盐噪声”（黑白噪点）效果最好。



**掩码越大 / 尺度越宽**：

- **平滑量增加**：图像变得更模糊，更多的细节被滤掉 。

  

- **去噪能力强**：能够去除更大、更强烈的噪声，因为参与平均的邻域像素更多了 。

  

- **高频丢失**：图像中的锐利边缘（高频成分）会被严重削弱，导致边缘定位不准 。

  


**掩码越小 / 尺度越窄**：

- **保留细节多**：图像看起来更接近原图，边缘更清晰。
- **去噪能力弱**：对噪声比较敏感，微小的干扰可能无法被完全平滑掉。



# 第二章

## 图像梯度

图像梯度反映了图像亮度的变化率。在数学上，它是图像函数在各个方向上的导数 。

## 为什么要计算梯度

**寻找变化**：梯度大的地方通常意味着亮度变化剧烈，代表了物体的边缘 。

**方向信息**：梯度不仅有大小（幅值），还有方向（指向亮度变化最快的方向） 。

怎么计算

有什么用：边缘计算

## Seam Carving 缝隙裁剪

这是一种智能调整图像尺寸的技术（不只是简单的缩放或裁剪）。

**核心原理**：

- **能量函数**：算法将**梯度**定义为“能量” 。梯度大的地方能量高（重要特征，如人脸），梯度小的地方能量低（不重要的背景，如天空）。
- **删减逻辑**：通过寻找并删除一条从上到下或从左到右、总能量最低的“路径”（Seam），从而在保留重要特征的同时改变图像大小 。



## 主要边缘检测步骤

1.平滑:抑制噪声
2.边缘增强:对比度过滤
3.边缘定位
确定滤波器输出中的哪些局部最大值实际上是边缘与噪声
阈值，细化



## canny边缘检测器

### 1. 噪声抑制：使用高斯滤波

在检测边缘之前，必须平滑图像以去除噪声。因为边缘检测依赖于导数计算，而导数对噪声极其敏感。

- **原理**：将图像与高斯内核进行卷积。

- 数学表达：

  若高斯函数为 $G(x, y) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2+y^2}{2\sigma^2}}$，则平滑后的图像 $S = G * I$。

- **影响**：$\sigma$（标准差）越大，平滑程度越高，细节（和小噪声）消失得越多。

------

### 2. 计算梯度的大小和方向

边缘是像素值发生剧烈变化的地方。我们通过计算图像在水平（$x$）和垂直（$y$）方向的导数来捕捉这种变化。

- **算子**：通常使用 **Sobel 算子** 卷积得到 $G_x$ 和 $G_y$。

- 梯度强度 ($M$)：表示边缘的锐利程度。

  

  $$M(x, y) = \sqrt{G_x^2 + G_y^2}$$

- 梯度方向 ($\theta$)：垂直于边缘走向的方向。

  $$\theta = \arctan\left(\frac{G_y}{G_x}\right)$$

------

### 3. 非极大值抑制 (Non-Maximum Suppression)

这一步是 Canny 算法变“瘦”的关键。上一步得到的梯度图往往在边缘处非常模糊（宽脊）。

- **目的**：保留局部梯度最大的点，剔除其余点，从而获得**细化的边缘**（1 像素宽）。
- **操作逻辑**：
  1. 将梯度方向 $\theta$ 离散化为 4 个基本方向（0°、45°、90°、135°）。
  2. 检查当前像素点与其梯度方向上的两个相邻点。
  3. 如果当前点的梯度强度 $M$ 不是这三个点中的最大值，则将其设为 0（抑制）。

------

### 4. 滞后阈值处理 (Hysteresis Thresholding)

为了解决噪声造成的边缘断裂，Canny 引入了**双阈值法**：

- **高阈值 (High Threshold)**：用于确定“强边缘”。如果像素梯度 > T_{high}，它被认为是确定可靠的边缘。
- **低阈值 (Low Threshold)**：用于确定“弱边缘”。如果像素梯度 < T_{low}，它被抛弃。
- **处于两者之间**：被标记为“待定”。

------

### 5. 边缘链接 (Edge Tracking by Hyteresis)

这是最后一步，通过逻辑判断来补全边缘：

- 查看所有的“弱边缘”像素。如果一个弱边缘像素在 8 邻域内连接到了一个“强边缘”像素，那么这个弱边缘就被“提拔”为边缘。



## 特征滤波的核心原理：卷积 (Convolution)

滤波器本质上是一个小型矩阵 3x3  /   5x5。

- **过程**：将这个小矩阵在图像上逐像素滑动。在每个位置，将滤波器中的权重与图像对应的像素值相乘并求和。
- **结果**：输出一张“特征图”。特征越明显的地方，输出值越高。

# 第三章

## 1. 什么是距离变换 (Distance Transform)?

距离变换主要作用于**二值图像**。它会将图像中的每一个像素值，替换为该像素到**最近背景像素（或边界）**的距离。

- **输入**：一张二值图（例如：白色是目标物体，黑色是背景）。
- **输出**：一张灰度图（距离图）。在目标中心的像素值很大（离边界远），而在目标边缘附近的像素值很小（接近 0）。

### 常见用途：

- **骨架提取**：找到物体的脊线（距离变换的最大值路径）。
- **分水岭算法的前处理**：用于分割相互重叠的圆形物体。
- **避障规划**：在机器人路径规划中，计算距离障碍物的安全距离。

------

## 2. 什么是 Chamfer 距离？

“Chamfer”一词原意为“倒角”。在图像处理中，它有两个层面的含义：

### A. 作为一种快速算法 (Chamfer Distance Transform)

直接计算每个点到最近点的**欧几里得距离**（直线距离）计算量很大。Chamfer 算法通过**局部掩码（Mask）迭代**的方式来近似这个距离。

- **基本思想**：通过两次扫描图像（从左上到右下，再从右下到左上），利用邻域像素的已知距离来更新当前像素的距离。
- **常用掩码**：
  - **3x3 掩码**：水平/垂直移动代价为 3，对角线移动代价为 4（近似 $\sqrt{2} \approx 1.414 \approx 4/3$）。
  - **5x5 掩码**：精度更高，更接近真实的圆形距离。

### B. 作为点集匹配指标 (Chamfer Distance Metric)

在深度学习和 3D 点云处理中，Chamfer 距离用于衡量两组点 $A$ 和 $B$ 之间的“差异”：

1. 对于 $A$ 中的每个点，找到 $B$ 中最近的点，求距离平方和。
2. 对于 $B$ 中的每个点，找到 $A$ 中最近的点，求距离平方和。
3. 将两者相加。

------

## 3. 两者的关系：Chamfer 匹配 (Chamfer Matching)

这是 Chamfer 距离最著名的应用场景。假设你想在图像中找一个特定形状（模板）：

1. 先对原图进行**边缘检测**，得到二值图。
2. 对该二值图进行**距离变换**（得到一张“距离地图”）。
3. 将你的模板（形状轮廓）覆盖在距离地图上。
4. **计算得分**：模板覆盖位置下，距离地图像素值的平均值。
   - 如果模板完全匹配，覆盖点的距离值都是 0，总得分也为 0。
   - 如果模板偏离了边缘，得分就会变大。



## **二值图像分析**

（步骤：预处理：形态学操作，腐蚀膨胀，开运算闭运算；连通区域分析）

**优点**
-计算速度快，易于存储
可用的处理技术简单
-产生一些有用的紧凑形状描述符
**缺点**
-难以获得“干净”的轮廓
现实场景中常见的噪音
-表示可能过于粗糙
-不是 3d

## 膨胀

扩展连通分量
增加特征
填补空洞

## 腐蚀

腐蚀连通分量
收缩特征
移除桥梁、分支、噪音

## 开运算

腐蚀，然后膨胀
移除小物体，保持原状

## 闭运算

膨胀,然后腐蚀
填补孔洞，但保持原状

## 连通分量

**连通分量**是指二值图像中由相同特性的像素（通常是白色，即值为 1）组成的相邻集合。简单来说，一个连通分量代表图像中的一个独立“物体”。

**4-连通 (4-Connectivity)**：只考虑像素的上下左右四个邻居。

**8-连通 (8-Connectivity)**：考虑上下左右以及四个对角线方向的邻居。

## 顺序连接算法

如何让计算机自动识别并给这些分量“编号”？最经典的方法是**两步扫描法（Two-Pass Algorithm）**。它不需要复杂的递归，效率很高。

### 第一步：初次扫描与标记 (First Pass)

从左到右、从上到下逐像素扫描图像。

1. **遇到背景像素 (0)**：跳过。
2. **遇到前景像素 (1)**：检查它左边和上边的邻居：
   - 如果邻居**都没有标签**：给当前像素分配一个全新的标签（如 Label 1）。
   - 如果**只有一个邻居有标签**：当前像素继承该标签。
   - 如果**两个邻居有不同标签**（例如左边是 1，上边是 2）：
     - 当前像素先继承其中一个标签。
     - **关键点**：记录标签 1 和标签 2 是“等价”的（它们其实属于同一个物体），存入一个**等价表 (Equivalence Table)**。

### 第二步：等价标签合并 (Second Pass)

再次扫描图像。

1. 查阅第一步建立的**等价表**。
2. 将所有属于同一等价组的标签统一修改为该组中的最小标签。
   - 例如：如果表中记录了 $1 \equiv 2$，那么图像中所有标记为 2 的像素都会被改为 1。

# 第四章

## 纹理

纹理相关的任务

### 从纹理估计形状

-从图像纹理估计表面方向或形状

### 根据纹理线索进行分割/分类

-分析，表示纹理
-将纹理一致的图像区域分组

### 合成

-根据一些示例生成新的纹理贴图/像

马尔科夫链：一个事件发生的概率，只取决于它前面的几个状态，而与更早的状态无关。

#### **•The Efros & Leung algorithm:过程 ** 用于纹理合成

##### 1. 准备阶段 (Initialization)

- **输入**：一张较小的**纹理样本图 (Input Sample)**。
- **输出**：建立一张空白的**目标画布**。
- **播种**：从样本图中随机选取一个小方块（例如 $3 \times 3$），放置在画布中间作为**起始种子 (Seed)**，作为生长的基础。

##### 2. 合成循环 (The Loop)

对于画布上每一个紧邻已知区域的**未知像素 $P$**，执行以下操作：

- A. 确定邻域 (Get Neighborhood)：

  截取像素 $P$ 周围的一个窗口（例如 $11 \times 11$）。此时窗口内包含部分已知像素（来自种子或已合成部分）和部分未知像素。

- B. 全局搜索 (Search)：

  拿着这个“半残”的窗口，去原始样本图中进行全图扫描。

- C. 计算距离 (Calculate SSD)：

  计算样本图中各个位置的窗口与当前 $P$ 的邻域窗口的误差平方和 (SSD)。注意：只比较那些已知位置的像素值。

- **D. 概率采样 (Probabilistic Sampling)**：

  - 找出 SSD 最小（最相似）的一组候选窗口（例如前 $K$ 个，或误差在一定阈值内的）。
  - 根据误差大小赋予概率（误差越小，概率越大），从中**随机抽取**一个窗口。

- E. 填补 (Fill)：

  将被抽中窗口的中心像素值赋值给像素 $P$。

##### 3. 迭代完成 (Iteration)

- 重复上述步骤，像“洋葱剥皮”的反向过程一样，一圈一圈向外扩展，直到填满整张画布。

#### 最小误差边界分析法

块的随机放置：直接把输入纹理的块（比如 B1、B2）随便拼在一起，结果就是下方最左图 —— 块和块之间边界明显，纹理不连贯。

受重叠约束的相邻块：让块之间有重叠区域，边界会稍微自然一点，但下方中间图还是能看到模糊的接缝。

最小误差边界切割：在重叠区域里，找 “像素误差最小” 的曲线来切割块，把 B1 和 B2 的边界换成这条曲线，这样块和块的纹理能完美衔接 —— 对应下方最右图，纹理连贯、几乎看不出拼接痕迹。



# 第五章

霍夫变换主要步骤和优缺点

## 霍夫变换

### 直线霍夫变换算法步骤 (Hough Transform Algorithm)

**1. 核心思想：极坐标参数化**

- 不使用斜率截距式 (y=kx+b)，而是使用极坐标方程来表示直线，以避免垂直线斜率无穷大的问题：

  
  $$
  x \cos \theta - y \sin \theta = d
  $$
  

  (注：这里 d 代表原点到直线的垂直距离)

**2. 算法执行流程**

- **Step 1: 初始化 (Initialize)**

  - 建立一个二维累加器数组 $H[d, \theta]$（Accumulator Array）。
  - 将所有位置的数值初始化为 0。

- **Step 2: 遍历边缘点 (Loop Edge Points)**

  - 对于图像中每一个被检测到的**边缘点** $I[x, y]$ 进行处理。

- **Step 3: 角度扫描与计算 (Angle Loop & Calculate)**

  - 对于当前点 $(x, y)$，遍历所有可能的角度 $\theta$（从 $\theta_{min}$ 到 $\theta_{max}$，按一定步长量化）。

  - 根据公式计算对应的距离 $d$：

    

    $$d = x \cos \theta - y \sin \theta$$

- **Step 4: 投票 (Vote)**

  - 在累加器对应的位置加一：$H[d, \theta] += 1$。
  - *理解：这意味着该 $(d, \theta)$ 参数代表的直线获得了一张“选票”。*

- **Step 5: 寻找峰值 (Find Maxima)**

  - 扫描累加器 $H$，找出票数最高（最大值）的那些 $(d, \theta)$ 组合。

- **Step 6: 输出结果 (Output)**

  - 这些峰值参数 $(d, \theta)$ 就对应了图像中检测到的直线。

### 圆的霍夫变换算法

**1. 核心目标**

- 在图像中检测圆形，确定其三个参数：**圆心坐标 (a, b)** 和 **半径 r**。
- 需要构建一个**三维累加器 (3D Accumulator)** H[a, b, r] 来存储投票结果。

**2. 基础算法流程 (Basic Algorithm)**

- **Step 1: 遍历边缘像素 (Edge Loop)**

  - 对图像中每一个边缘点 (x, y) 进行处理。

- **Step 2: 遍历半径 (Radius Loop)**

  - 对每一个可能的半径值 r 进行尝试。

- **Step 3: 遍历角度 (Angle Loop - 暴力法)**

  - 对每一个可能的梯度方向 
    $$
    \theta
    $$
    （从 0 到 360 度）进行遍历。

  - *注：这意味着假设圆心可能在当前点的任意方向。*

- **Step 4: 计算圆心并投票 (Vote)**

  - 根据公式计算假设的圆心 (a, b)：

    
    $$
    a = x + r \cos(\theta)
    
    b = y - r \sin(\theta)
    $$
    

    (注：公式含义为从当前点 $(x, y)$ 沿方向 $\theta$ 移动距离 $r$ 到达圆心)

  - 在累加器对应位置加一：$H[a, b, r] += 1$。

**3. 关键优化 (Optimization)**

- **利用梯度信息 (Use Gradient)**
  - **方法**：直接使用边缘点 $(x, y)$ 处的**估计梯度方向**作为 $\theta$，而不是遍历所有角度。
  - **效果**：消除了最内层的角度循环，将计算复杂度从 $O(\text{像素} \times \text{半径} \times \text{角度})$ 降低为 $O(\text{像素} \times \text{半径})$。
  - **原理**：圆上任意一点的梯度方向（法线）必然指向圆心。

**4. 结果输出**

- 在三维累加器 $H[a, b, r]$ 中寻找**局部峰值 (Maxima)**，对应的 $(a, b, r)$ 即为检测到的圆。

### 技巧

#### 1. 首先最小化不相关的标记 (Minimize irrelevant tokens/markers first)

- **含义**：在进行投票之前，先清理数据。
- **解释**：霍夫变换是对边缘点进行投票。如果你的边缘检测图（Edge Map）里有很多噪点（比如纹理造成的细碎边缘，或者孤立的噪声点），它们会产生大量的无效投票，干扰真正的峰值检测。
- **做法**：在使用 Canny 边缘检测时，设置更高的阈值，或者在投票前进行形态学滤波（如腐蚀操作）去掉孤立噪点，只保留那些看起来像真正轮廓的边缘点。

#### 2. 选择一个好的网格/离散化 (Choose a good grid/discretization)

- **含义**：累加器数组（Accumulator Array）的格子大小（Bin size）非常关键，需要权衡。
- **解释**：
  - **太细 (Too fine)**：如果你把角度步长设为 0.01 度，或者距离步长设为 0.1 像素。
    - *后果*：由于图像本身的像素化误差，本该投给同一个格子的票会分散到周围邻近的格子里。导致最高票数变低，很难找到明显的峰值。同时计算量和内存消耗巨大。
  - **太粗 (Too coarse)**：如果你把角度步长设为 10 度。
    - *后果*：精度太差。两条靠得很近但不重合的直线可能会被归入同一个格子里，被误认为是一条线。

#### 3. 也为邻居投票 (Vote for neighbors too)

- **含义**：也就是**平滑处理 (Smoothing)** 或 **软投票 (Soft Voting)**。
- **解释**：由于噪声或量化误差，一个边缘点计算出的 $(\rho, \theta)$ 可能稍微偏了一点点，没投进它该进的格子，而是投进了隔壁格子。
- **做法**：当你给 $(d, \theta)$ 这个格子加 1 分时，顺便也给它周围的格子（比如 3x3 邻域）加上一点分数（例如 0.5 分），或者在投票结束后对整个累加器矩阵做一次高斯平滑。这样可以让真正的峰值变得更平滑、更显著，抗噪能力更强。

#### 4. 利用边缘的方向来减少 1 个参数 (Use edge direction to reduce 1 parameter)

- **含义**：这是最重要的**加速技巧**（刚才在“圆的霍夫变换”中提到过）。
- **解释**：
  - **通常做法**：对于一个点，不知道线/圆的方向，所以要遍历所有可能的角度（360度）。
  - **优化做法**：利用梯度算子（如 Sobel）先计算出该边缘点的**梯度方向**。因为边缘的法线方向（梯度方向）通常就是几何形状参数所在的方向（圆指向圆心，直线指向垂足）。
  - **结果**：你不再需要遍历 $\theta$，直接锁定方向。这能把计算复杂度降低一个维度（例如找圆从 $O(N^3)$ 降到 $O(N^2)$）。

#### 5. 为了读取哪些点投票给“获胜”的峰值，请在投票时保留标签 (Keep tags to retrieve contributing points)

- **含义**：不仅仅记录“有多少人投票”，还要记录“是谁投的票”。
- **解释**：
  - 标准的霍夫变换只告诉你“这里有一条直线”。但它**不告诉你**这条直线是由图像上的哪几个像素组成的，也不告诉你直线的**起止点**在哪里（它默认直线是无限长的）。
  - **做法**：在累加器的格子里，不仅存一个计数器（Int），还存一个列表（List），把投票给这个格子的所有边缘点坐标 $(x, y)$ 都记下来。
  - **用途**：当你确定这个格子是峰值后，查阅这个列表，就能知道原图中哪些点构成了这条线。这样你就可以通过分析这些点的分布来确定线段的端点，或者把这些点从图像中剔除以便进行下一轮检测。

### 霍夫变换:优缺点

 • 优点 1、所有点都是独立处理的，因此可以应对遮挡和间隙问题； 2、对噪声有一定的鲁棒性：噪声点不太可能持续对任何一 个单一的区间产生贡献。 3、可以在一次遍历中检测到模型的多个实例 

• 缺点 1、搜索时间的复杂度随着模型参数数量的增加而呈指数增 长 2、非目标形状可能会在参数空间中产生虚假的峰值 3、量化：选择一个合适的网格大小可能很困难

# 第六章

分割——聚类、图切割？

kmeans mean-shift重点：实现步骤，

## 基于聚类的自下而上分割

### K-Means

 优点和缺点 

优点 • 简单，计算速度快 • 收敛到聚类内平方误差的局部最小值 

缺点/问题 • 如何设置 k? • 对初始中心点敏感 • 对异常值敏感 • 检测球形聚类 • 假设可以计算均值

### 均值漂移Mean Shift

**步骤：**

**初始化窗口 (Initialization)**：

- 在特征空间中选择一个点作为起始中心。
- 定义一个半径为 $h$（带宽/Bandwidth）的窗口（通常是圆形或球形核函数）。

**计算均值 (Compute Mean)**：

- 计算窗口内所有数据点的**质心**（即加权平均值）。这代表了窗口内密度最高的方向。

**漂移 (Shift)**：

- 将窗口的中心**移动**到刚刚计算出的质心位置。
- *这一步就是“均值漂移”名称的由来：中心向着数据更密集的地方“漂移”。*

**迭代 (Iteration)**：

- 重复步骤 2 和步骤 3，直到窗口的中心不再移动（或者移动距离非常小），即收敛到了**局部密度峰值（Mode）**。

**聚类 (Clustering)**：

- 对所有数据点重复上述过程。
- 所有最终收敛到同一个峰值的点，被归类为同一个簇（Cluster）。

优点: • 不对簇的形状进行假设 • 只需选择一个参数（窗口大小，亦称 “带宽”） • 通用技术 • 可找到多个模式 

• 缺点: • 需要选择窗口大小 • 随着特征空间维度的增加，不易扩展

### 图切割

用数学语言重新描述图片：

- **节点 (Nodes, $V$)**：图像中的每一个像素点就是一个节点。
- **边 (Edges, $E$)**：连接两个像素的线。通常我们只连接相邻的像素，或者距离在一定范围内的像素。
- **权重 (Weights, $W$)**：每条边都有一个分值，代表两个像素的**相似度**（亲和力）。
  - 如果你和邻居颜色很像（比如都是蓝天），权重就很大（不容易剪断）。
  - 如果你是黑色，邻居是白色（边缘处），权重就很小（容易剪断）。

| **算法**               | **核心目标**               | **惩罚项（分母）**          | **倾向性**     | **缺点**                 |
| ---------------------- | -------------------------- | --------------------------- | -------------- | ------------------------ |
| **最小割 (Min-Cut)**   | 切断的边权重和最小         | 无                          | **切出孤立点** | 喜欢切角落，无法均衡分割 |
| **比率割 (Ratio Cut)** | 权衡切割代价与**节点数量** | 子图的**节点数** ($         | A              | $)                       |
| **归一化切割 (N-Cut)** | 权衡切割代价与**连接总量** | 子图的**权重和** ($Vol(A)$) | **密度平衡**   | 计算量大 (特征值分解慢)  |

# 第七章

局部特征匹配

## 局部不变特征检测

![image-20260117164929074](C:/Users/lenovo/AppData/Roaming/Typora/typora-user-images/image-20260117164929074.png)

### 1. 核心流程 (The Pipeline)

通常包含以下三个主要阶段：

- **第一步：特征检测 (Detection) —— “找点”**
  - **目标**：在图像中找到具有丰富信息的**关键点 (Keypoints)**（如角点、斑点）。
  - **关键技术**：构建**尺度空间 (Scale Space)**（如高斯金字塔），确保无论是大图还是小图，都能找到同一个点（解决**缩放**问题）。
- **第二步：特征描述 (Description) —— “描述”**
  - **目标**：提取关键点周围的局部图像信息，生成一个数字向量（描述子）。
  - **关键技术**：
    - **方向赋值**：计算局部梯度的方向，将坐标系旋转到主方向（解决**旋转**问题）。
    - **直方图统计**：统计局部区域的梯度分布，生成向量（如 SIFT 的 128 维向量），以增强对**光照**变化的鲁棒性。
- **第三步：特征匹配 (Matching) —— “配对”**
  - **目标**：在两张图像的特征向量集合中，寻找彼此距离最近的配对。
  - **方法**：通常使用欧氏距离或汉明距离（针对二进制描述子）。

------

### 2. 优缺点分析

**优点 (Pros):**

- **不变性 (Invariance)**：这是最大的优势。对图像的**缩放 (Scale)、旋转 (Rotation)、平移**具有高度不变性，对**光照 (Illumination)** 和**视角 (Viewpoint)** 变化也有较强的抵抗力。
- **局部性 (Locality)**：特征是局部的，即使物体被**遮挡 (Occlusion)** 一部分，只要剩下的部分特征点足够，依然能识别。
- **独特性 (Distinctiveness)**：单个特征点的描述信息量大，区分度高，适合在海量数据库中进行匹配。

**缺点 (Cons):**

- **计算量大 (Computationally Expensive)**：经典的 SIFT/SURF 算法涉及大量的高斯卷积和浮点运算，速度较慢，难以在低端设备上实时运行。
- **对平滑区域无效**：在天空、墙壁等纹理缺乏的区域无法检测到特征点。
- **重复纹理困扰**：如果图像中有大量重复图案（如棋盘格、栅栏），描述子会非常相似，导致匹配时产生歧义。



### 特征检测

1. **Harris** 负责找“角点”（位置好找，但不知道大小）。
2. **LoG** 负责找“斑点”（对大小敏感）。
3. **自动尺度选择** 负责解决“到底该看多大范围”的问题，把前两者结合起来，实现尺度不变性。

以下是详细解释：

------

#### 1. 哈里斯角点检测器 (Harris Corner Detector)

**核心任务：找到图像中“最独特”的点（角点）。**

- 直观直觉：

  想象你手里拿一个小方框（窗口），在图像上滑动。

  - **平坦区域 (Flat)**：如果在墙面上滑动，怎么滑框里的内容都差不多（像素变化小）。
  - **边缘区域 (Edge)**：如果在桌子边沿滑动，顺着边滑没变化，垂直边滑变化很大。
  - **角点区域 (Corner)**：如果在桌角滑动，**无论往哪个方向滑**，框里的像素都会剧烈变化。
  - **结论**：Harris 检测器就是找这种“往任何方向移动，像素值都剧烈变化”的点。

- 数学原理：

  它计算一个自相关矩阵 (Auto-correlation Matrix, $M$)。通过分析这个矩阵的两个特征值 ($\lambda_1, \lambda_2$)：

  - $\lambda_1 \approx 0, \lambda_2 \approx 0$ $\rightarrow$ 平坦。
  - $\lambda_1 \gg 0, \lambda_2 \approx 0$ $\rightarrow$ 边缘。
  - **$\lambda_1$ 和 $\lambda_2$ 都很大** $\rightarrow$ **角点**（这是我们要的）。

- **优缺点**：

  - **优点**：对**旋转**是不变的（转了还是角点）。
  - **缺点**：对**尺度（缩放）**非常敏感。一张图被缩小后，原来的角点可能变得很尖，也被看作角点；但如果图被放大很多倍，角点可能看起来像一条平滑的曲线，Harris 就检测不到了。

------

#### 2. 高斯拉普拉斯 (Laplacian of Gaussian, LoG)

**核心任务：找到图像中的“斑点” (Blob)。**

- **直观直觉**：

  - **高斯 (Gaussian)**：先给图像做模糊，去除噪点。
  - **拉普拉斯 (Laplacian)**：求二阶导数。二阶导数对“剧烈突变”非常敏感。
  - **LoG**：合起来就是“先模糊再求二阶导”。它的形状像一个**墨西哥草帽 (Mexican Hat)**——中间突起，周围凹陷。
  - 当这个“草帽”盖在图像上的一个**亮斑**（中间亮周围暗）上，且**草帽的大小和光斑的大小刚好吻合**时，它的响应值最大。

- 作用：

  Harris 找角点，LoG 找斑点（比如向日葵的花盘、圆形的灯）。最重要的是，LoG 对尺寸非常敏感。

------

#### 3. 自动尺度选择 (Automatic Scale Selection)

**核心任务：解决“我怎么知道这个物体有多大？”的问题。**

- 问题背景：

  Harris 检测器用的是固定大小的窗口。但是现实世界里，同一个物体可能离得近（显得大），也可能离得远（显得小）。如果我们只用固定窗口，就没法匹配这两种情况。

- **解决方案：尺度空间 (Scale Space)**

  1. **构造金字塔**：把图像不断地做高斯模糊（模拟远近距离），产生一系列不同模糊程度（尺度 $\sigma$）的图像。
  2. **寻找极值**：对于图像上的某一个位置，我们用不同大小的 LoG 滤波器（或者它的近似 DoG）去检测。
  3. **特征尺度**：
     - 如果 $\sigma$ 太小，滤波器只盖住了斑点的一小部分，响应低。
     - 如果 $\sigma$ 太大，滤波器把斑点和背景混在了一起，响应也低。
     - **只有当 $\sigma$ 与斑点的实际大小完全匹配时**，LoG 的响应达到**峰值**。
  4. **结论**：这个峰值对应的 $\sigma$，就是该特征点的**“特征尺度”**。

- 最终效果：

  不管你的照片是放大的还是缩小的，算法都能通过寻找峰值，自动算出这个特征点原本的大小（比如“这个斑点半径是 5 个像素” vs “这个斑点半径是 50 个像素”）。然后把它们统一缩放到一样的大小进行描述，从而实现了尺度不变性 (Scale Invariance)。

#### SIFT

##### **第一步：尺度空间极值检测 **

- **目标**：解决**缩放 (Scale)** 问题。
- **方法**：使用 **高斯差分 (DoG)** 金字塔。
- **原理**：在不同的模糊程度（尺度）和分辨率下寻找像“斑点”一样的极值点。这确保了无论物体是远是近，都能在对应的尺度层上找到它。

##### **第二步：关键点定位 **

- **目标**：精确定位，去伪存真。
- **方法**：
  - 通过泰勒展开对极值点位置进行亚像素级精修。
  - **去除低对比度点**（怕噪点）。
  - **去除边缘响应点**（消除 Harris 角点检测器那种对边缘敏感的问题，利用类似 Hessian 矩阵的方法）。

##### 第三步：方向分配

- **目标**：解决**旋转 (Rotation)** 问题。
- **方法**：计算关键点邻域梯度的直方图，找到峰值方向，将坐标系旋转至该方向。

##### 第四步：关键点描述 

- **目标**：生成独一无二的“身份证”。
- **方法**：
  - 在旋转后的坐标系中，取 $16 \times 16$ 的邻域。
  - 划分为 $4 \times 4$ 个子区域。
  - 计算每个子区域的 8 方向梯度直方图。
  - 输出 **128 维向量**。
  - **归一化**处理以抵抗光照变化。

# 第八章

极线几何约束

## 极线约束

![image-20260117212447435](C:/Users/lenovo/AppData/Roaming/Typora/typora-user-images/image-20260117212447435.png)

![image-20260117212603433](C:/Users/lenovo/AppData/Roaming/Typora/typora-user-images/image-20260117212603433.png)

## 立体对应（Stereo Correspondence）

### 1. 什么是立体对应？

立体对应的核心目标是在左图和右图中找到**同一个物理点**的像素位置。找到了对应点，我们就能计算**视差（Disparity）**，进而推算出深度信息。

然而，仅仅依靠像素颜色去搜索（比如“左边的红点对应右边的哪个红点？”）是非常不可靠的，因为图像中可能有很多相似的红点，或者存在噪声。

### 2. 核心约束：寻找对应点的规则

为了准确找到对应点，算法必须遵循一系列“约束条件”来排除错误匹配：

- **硬约束 (Hard Constraint)**：极线约束（Epipolar Constraint），即点只能在对应的极线上找。
- **软约束 (Soft Constraints)**：
  - **相似性 (Similarity)**：匹配点在颜色、亮度上应非常接近。
  - **唯一性 (Uniqueness)**：左图的一个点最多对应右图的一个点。
  - **顺序性 (Ordering)**：如果物体 A 在物体 B 的左侧，那么在左右两个视图中，它们的相对顺序应该保持一致（这对于互不遮挡的表面成立）。
  - **视差梯度 (Disparity Gradient)**：深度通常是平滑变化的，不会随意剧烈跳动。

------

### 3. 算法流程一：基于动态规划 (Dynamic Programming)

这是一种**逐行优化 (Scanline Optimization)** 的方法。它不单独看某个像素，而是试图让“这一整行”的匹配代价最小。

#### **步骤流程：**

1. **构建网格**：建立一个二维网格，纵轴代表左扫描线，横轴代表右扫描线。
2. **寻找路径**：目标是从网格的左上角（起点）走到右下角（终点），寻找一条**代价最小的路径 (Shortest Path)**。
3. **定义移动代价**：
   - **对角线移动 (Sequential/Match)**：表示左图像素 $x$ 与右图像素 $y$ 匹配。代价 = 像素颜色差异（差异越小代价越低）。
   - **水平/垂直移动 (Occluded/Disoccluded)**：表示出现了遮挡。即某个像素在另一张图中看不到。此时需要付出固定的“遮挡惩罚”代价。
4. **回溯**：找到总代价最小的路径后，路径上的每一个节点就对应一个确定的匹配关系。

- **优点**：解决了行内的顺序和遮挡问题。
- **缺点**：行与行之间没有联系，生成的深度图容易出现水平条纹（Streaking artifacts）。

------

### 4. 算法流程二：基于图割 (Graph Cuts)

这是一种**全局优化 (2D Grid Optimization)** 的方法。它将立体匹配视为一个**能量最小化 (Energy Minimization)** 问题。

#### **核心公式：能量函数**

$$E = \alpha E_{data} + \beta E_{smooth}$$

- **$E_{data}$ (数据项)**：描述匹配的好坏。即像素 $I_1(i)$ 和它匹配的 $I_2(i+D(i))$ 颜色有多像。差异越小，能量越低。
- **$E_{smooth}$ (平滑项)**：描述深度的连续性。相邻像素的视差 $D(i)$ 和 $D(j)$ 应该相近。如果深度突然跳变（除非是边缘），能量就会变高。

#### **步骤流程：**

1. **构建图 (Graph Construction)**：将图像中的每个像素视为图的一个节点，节点之间连接边。
2. **定义权重**：根据能量函数设定边的权重（数据项对应节点与源/汇的连接，平滑项对应节点间的连接）。
3. **最大流最小割 (Max-Flow Min-Cut)**：使用图割算法切断图的连接。
   - “切割”的代价等于能量 $E$。
   - 找到一种切法，使得总代价（能量）最小。
4. **生成视差图**：切割的结果直接告诉我们需要给每个像素分配哪个视差值（$D$）。

- **优点**：考虑了图像的二维结构，生成的深度图非常平滑自然，效果通常优于动态规划。

### 总结

立体对应算法的进化路径是：

从单个像素匹配（容易出错） $\rightarrow$ 单行联合优化（动态规划，解决遮挡和顺序） $\rightarrow$ 全图联合优化（图割，解决全局平滑性）。

# 第九章

实例识别——倒排，词袋相似性

## 倒排文件索引

### **1. 核心概念**

- **定义**：一种为了在大规模数据库中实现**快速检索**而设计的数据结构。
- **核心逻辑**：**“以特征找图”**。
  - *正向索引*：图片 ID $\rightarrow$ 包含的特征（像书的目录）。
  - *倒排索引*：特征 ID $\rightarrow$ 包含该特征的图片 ID 列表（像书后的索引页）。

### **2. 直观理解**

- **类比**：**书籍的索引页**。
- 如果你想找“DNA”这个词在书里哪里出现，你不需要从头读到尾（遍历），而是直接翻到书后的索引，找到“DNA”对应的页码列表（如：第 5, 12, 118 页）。

### **3. 工作流程**

- **阶段一：建立索引（离线/预处理）**
  - 提取数据库中所有图片的特征（视觉词）。
  - 为每一个**视觉词 (Visual Word)** 建立一个**倒排表 (Posting List)**，记录所有包含该视觉词的图片 ID。
  - *结构示例*：`特征A -> [图片1, 图片3, 图片9...]`
- **阶段二：搜索（在线/实时）**
  - 提取查询图片的特征（例如：特征 A）。
  - **查表**：直接在索引中找到“特征 A”对应的倒排表。
  - **获取候选**：仅对倒排表中的图片（图片 1, 3, 9...）进行进一步匹配，**忽略**数据库中其他不含该特征的图片。

### **4. 核心优势**

- **解决稀疏性 (Sparsity)**：查询图片通常只包含词典中极少量的特征。
- **极高效率**：将检索的时间复杂度从 $O(N)$（N为数据库总图数）降低为与**包含特定特征的图片数量**相关。
- **应用**：是大规模图像检索（如 Bag of Words 模型）和文本搜索引擎的基石技术。

## 词袋模型 (Bag of Words) 

### **1. 核心概念**

- **定义**：一种将图像表示为**固定长度数值向量（直方图）**的方法。
- **核心逻辑**：**“只看有什么，不看在哪里”**。
  - 将图像看作是一个装满“视觉单词”的袋子。
  - **忽略**特征的空间位置信息。
  - **只统计**各类特征出现的频率（次数）。
- **类比**：文本检索。统计一篇文章中“苹果”、“香蕉”出现的次数，而不关心它们在句子中的顺序。

### **2. 标准处理流程 (The Pipeline)**

- **第一步：特征提取 (Feature Extraction)**
  - 从图像中提取局部特征（通常使用 **SIFT**）。
  - *状态*：得到一堆杂乱的特征向量。
- **第二步：构建词典 (Codebook Generation)**
  - **方法**：使用 **K-Means** 聚类算法对训练集中所有的特征进行聚类。
  - **结果**：聚类中心 (Cluster Centers) 被定义为**“视觉单词” (Visual Words)**，所有中心构成了“词典”。
- **第三步：量化 / 编码 (Quantization)**
  - 将新图像的每一个特征点，映射到距离它最近的那个“视觉单词”上。
- **第四步：生成直方图 (Histogram Generation)**
  - 统计整张图中每个“视觉单词”出现的次数。
  - *输出*：一个固定长度的向量（例如：[10, 5, 0...]，表示单词1出现10次，单词2出现5次...）。

### **3. 优缺点分析**

- **优点 (Pros)**
  - **格式统一**：将不同尺寸、不同复杂度的图像都转换成了固定长度的向量，便于输入机器学习模型（如 SVM）。
  - **鲁棒性**：对**物体遮挡 (Occlusion)** 和背景混乱有较强的抵抗力（因为只统计局部特征的存在性）。
- **缺点 (Cons)**
  - **丢失空间信息 (No Spatial Info)**：这是最大缺陷。打乱图像块的位置，生成的直方图完全一样（无法区分“人脸”和“五官打乱的人脸”）。
  - *改进方案*：**空间金字塔匹配 (Spatial Pyramid Matching)** —— 将图像切块分别统计，引入粗略的空间位置信息。

---



关于 **空间验证 (Spatial Verification)** 的两种核心策略（RANSAC 与 广义霍夫变换）

## 1. RANSAC (随机采样一致性)

**核心逻辑**：自顶向下的**假设-验证**机制。通过不断的随机试错，寻找能够解释最多数据的那个模型。

- **步骤流程**：
  1. **随机采样 (Random Sample)**：从匹配点集中随机选择最少数量的点（如计算单应性矩阵需 4 对）。
  2. **模型构建 (Model Hypothesis)**：假设这几个点是正确的，计算出几何变换参数（如矩阵 $H$）。
  3. **验证共识 (Verify Consensus)**：用该参数测试剩余所有点，统计有多少点符合该模型（即 **Inliers/内点** 数量）。
  4. **循环迭代 (Loop)**：重复上述步骤 $N$ 次。
  5. **择优 (Best Fit)**：保留内点数量最多的那个模型作为最终结果。
- **优点 (Pros)**：
  - **精度极高**：能计算出非常精确的几何变换参数。
  - **鲁棒性强**：即使存在大量错误匹配（Outliers > 50%），依然能找到正确结果。
- **缺点 (Cons)**：
  - **速度慢**：带有随机性，如果内点比例低，需要极大的迭代次数才能保证找到解。
  - **单目标限制**：一次通常只能拟合一个主导模型，难以同时处理多个不同的物体。

------

## 2. 广义霍夫变换 (Generalized Hough Transform, GHT)

**核心逻辑**：自底向上的**投票**机制。让每个局部特征独立预测物体中心，通过聚类寻找“热点”。

- **步骤流程**：
  1. **建立索引 (Offline)**：记录每个特征点相对于物体中心的相对位置（距离、角度），存入 R-table。
  2. **特征匹配 (Matching)**：在查询图中找到特征点。
  3. **反向投票 (Voting)**：每个特征点根据 R-table 中的记录，向参数空间（Hough Space）中它认为物体中心可能出现的位置投一票。
  4. **寻找峰值 (Peak Finding)**：在参数空间中寻找票数最高的位置（局部峰值），即为物体位置。
- **优点 (Pros)**：
  - **多目标检测**：可以同时在图像中检测出多个相同的物体（会有多个峰值）。
  - **抗遮挡**：部分特征被遮挡不影响其他特征的投票，峰值依然存在。
- **缺点 (Cons)**：
  - **资源消耗大**：参数空间维度高（位置+尺度+旋转 = 4维），内存和计算开销巨大。
  - **精度受限**：受限于分箱（Bin）的大小，定位精度通常不如 RANSAC 精细。

------

## 3. 核心对比总结 (One-Liner)

- **RANSAC**：像**独裁者**。随机选几个亲信定规矩，看大家服不服。适合**精细验证**和**单目标**。
- **广义霍夫变换**：像**民主投票**。所有人把票投进箱子，选票数最高的。适合**初步筛选**和**多目标**。

## 精确率

已经查找的结果中多少是正确的

## 召回率

所有正确结果中找出了多少个

# 第十章

基于窗口的检测方法

##  **Viola-Jones (VJ) 算法**

### 第一阶段：特征提取 (准备素材)

1. **定义特征**：使用 **Haar-like 矩形特征**（如“上黑下白”、“左黑右白”的矩形块）来描述图像的局部纹理。
2. **加速计算**：构建 **积分图 (Integral Image)**。通过积分图，无论特征形状多大，都能在常数时间内（$O(1)$）快速计算出像素和，为后续的海量计算打下基础。

------

### 第二阶段：AdaBoost 训练 (核心筛选)

这是 VJ 算法的“灵魂”，它的目标是从 160,000+ 个候选特征中，挑选出极少数（几千个）最有效的特征，并组合成一个强分类器。

**流程步骤如下：**

1. **初始化**：给训练集中所有的正样本（人脸）和负样本（非人脸）分配**相同**的权重。
2. **迭代训练 (循环 T 轮)**：
   - **竞选 (Select Best Weak Classifier)**：遍历所有候选特征，找到在当前样本权重下，**分类错误率最低**的那个特征（即本轮的“最佳弱分类器”）。
   - **权重调整 (Update Weights)**：
     - **分错了的样本**：**增加权重**。让算法在下一轮“重点关注”这些难啃的骨头。
     - **分对了的样本**：降低权重。
   - **分配话语权**：根据该弱分类器的表现，给它分配一个投票权重（表现越好，在最终表决时说话分量越重）。
3. **线性组合**：将这就选出的 $T$ 个弱分类器进行加权线性组合，形成最终的 **强分类器 (Strong Classifier)**。

------

### 第三阶段：级联检测 (快速实战)

为了解决滑动窗口计算量大的问题，采用“过筛子”的策略。

1. **构建级联结构 (Cascade)**：将训练好的强分类器串联起来。
   - **前级**：包含很少的特征（甚至只有2个），非常简单快速，专门用来**快速排除**明显的非人脸背景。
   - **后级**：包含较多特征，复杂且慢，用来做精细判断。
2. **滑动窗口检测**：
   - 窗口在图像上滑动。
   - 如果一个窗口在**任意一级**被判定为“非人脸”，直接抛弃，不再进入下一级。
   - 只有通过了**所有**级联关口的窗口，才被最终判定为“人脸”。



识别，检测，跟踪，区域识别，多模态大模型：任务定义（输入输出），实现的整体框架，应用场景

常见的网络：AlexNet transformer resnet 每层的作用，自注意力互注意力

## CNN

1. ### **卷积层 (Convolution Layer)**

   - **作用**：**特征提取**。
   - **原理**：利用学习到的滤波器（Kernel）在图像上滑动，提取图像中的边缘、纹理、形状等局部特征。
   - **地位**：是网络中最核心的线性运算层。

2. ### **非线性层 / 激活层 (Non-linearity)**

   - **作用**：**引入非线性变化**。
   - **原理**：通常跟在卷积层之后，使用激活函数（如 ReLU）。如果不加这一层，网络无论多少层都只是线性叠加，无法模拟现实世界中复杂的非线性关系。

3. ### **空间池化层 (Spatial Pooling)**

   - **作用**：**降维与抗干扰**。
   - **原理**：进行下采样（如最大池化 Max Pooling）。它减少了特征图的分辨率和参数量（降低计算成本），同时让特征对微小的位移和变形具有不变性（Invariance）。

4. ### **归一化层 (Normalization)**

   - **作用**：**加速收敛与稳定训练**。
   - **原理**：对数据进行标准化处理（如 Batch Normalization），防止数据分布在层与层传递时发生剧烈变化（Internal Covariate Shift）。

### 常见神经网络

#### 1. AlexNet (深度学习的引爆点)

- **核心特点**：
  - **结构加深**：相比早期的 LeNet，它使用了更深的网络结构（8层）。
  - **引入 ReLU**：首次成功使用 ReLU 激活函数代替 Sigmoid/Tanh，解决了梯度消失问题，大大加快了训练速度。
  - **Dropout**：引入 Dropout 层来防止过拟合。
  - **GPU 加速**：开创了使用 GPU 并行训练神经网络的先河。

#### 2. ResNet (残差网络 - 解决“深”的问题)

- **地位**：解决了“网络越深越难训练”的世纪难题，让网络可以轻松达到上百层甚至上千层。
- **核心特点**：
  - **残差块 (Residual Block)**：这是它的灵魂。
  - **跳跃连接 (Skip Connection)**：它不强求网络直接学习目标映射 H(x)，而是学习残差 F(x)=H(x)−x。
  - **直观理解**：通过一条“短路”线，把上一层的输入直接加到下一层的输出上。这使得梯度可以无损地反向传播，彻底解决了深层网络中的梯度消失和退化问题。

#### 3. Transformer (注意力机制 )

- **地位**：最初用于 NLP（自然语言处理），后来通过 Vision Transformer (ViT) 席卷了计算机视觉领域，是目前大模型（如 GPT）的基石。
- **核心特点**：
  - **抛弃卷积 (No Convolution)**：它不像 CNN 那样通过局部窗口滑窗，而是直接处理序列。
  - **自注意力机制 (Self-Attention)**：这是核心。它让模型能够关注输入数据中任意两个位置之间的关系（全局感受野）。
    - *CNN 看图*：先看局部，再慢慢组合。
    - *Transformer 看图*：一眼看全局，并计算“像素 A”和“像素 B”之间有多大关联。
  - **并行计算**：相比 RNN（循环神经网络），它可以并行处理整个序列，训练效率极高。

### 激活函数，损失函数

### 第一部分：常见的激活函数 (Activation Functions)

我们将激活函数分为两类讨论：**隐藏层**（中间层）和**输出层**。

#### 1. 用于“隐藏层” (Hidden Layers)

*目的是让神经网络具备学习非线性关系的能力。*

- **ReLU (Rectified Linear Unit)**
  - **公式**：$f(x) = \max(0, x)$
  - **特点**：正数保持不变，负数归零。
  - **应用场景**：**几乎所有现代深度神经网络（CNN, ResNet, Transformer）的首选默认激活函数。**
  - **理由**：计算极快（只有比较运算），且在正区间解决了梯度消失问题，收敛速度快。
  - **缺点**：存在“Dead ReLU”问题（如果输入一直是负数，神经元就彻底“死”了，再也不更新）。
- **Leaky ReLU / PReLU**
  - **特点**：在负数区间给一个很小的斜率（不再是死绝的 0）。
  - **应用场景**：当你发现网络中有很多神经元不再更新（“死”掉了），或者为了提升一点点精度时使用。GAN（生成对抗网络）中常用。
- **Tanh (双曲正切)**
  - **特点**：输出范围在 $(-1, 1)$ 之间，以 0 为中心。
  - **应用场景**：**循环神经网络 (RNN)** 或 **LSTM** 中常用。
  - **理由**：相比 Sigmoid，它是零中心的（Zero-centered），收敛稍好，但在深层网络中依然有梯度消失风险。
- **Sigmoid**
  - **特点**：输出范围 $(0, 1)$。
  - **应用场景**：**现在极少用于隐藏层**。
  - **理由**：容易导致严重的**梯度消失 (Vanishing Gradient)**，且涉及指数运算，计算慢。

#### 2. 用于“输出层” (Output Layer)

*目的是将神经网络的输出转化为我们需要的格式。*

- **Sigmoid**
  - **应用场景**：**二分类问题** (Binary Classification)。
  - **含义**：输出一个 0 到 1 的概率值（例如：是猫的概率是 0.8）。
- **Softmax**
  - **应用场景**：**多分类问题** (Multi-class Classification)。
  - **含义**：输出一组概率分布，所有类别的概率加起来等于 1（例如：猫 0.7，狗 0.2，鸟 0.1）。
- **Linear (或者不加激活函数)**
  - **应用场景**：**回归问题** (Regression)。
  - **含义**：需要预测具体的数值（如房价、温度、坐标）时，输出层通常不加非线性激活。

------

### 第二部分：常见的损失函数 (Loss Functions)

损失函数的选择完全取决于你的**任务类型**。

#### 1. 回归任务 (预测连续数值)

*任务举例：预测房价、预测物体中心点坐标、预测股票价格。*

- **MSE (均方误差 / L2 Loss)**
  - **特点**：计算预测值与真实值之差的平方。
  - **应用场景**：**最标准的回归损失函数**。
  - **注意**：对**异常值 (Outliers)** 非常敏感。因为误差被平方了，一个巨大的错误会被放大很多倍，拉偏模型。
- **MAE (平均绝对误差 / L1 Loss)**
  - **特点**：计算预测值与真实值之差的绝对值。
  - **应用场景**：数据中**噪音或异常值较多**的时候。它比 MSE 更鲁棒（Robust）。
- **Huber Loss / Smooth L1**
  - **应用场景**：**目标检测 (Object Detection)** 中的边界框回归（如 Faster R-CNN, YOLO）。
  - **特点**：结合了 MSE 和 MAE 的优点。误差小的时候用 MSE（收敛快），误差大的时候用 MAE（抗干扰）。

#### 2. 分类任务 (预测类别)

*任务举例：识别图片是猫还是狗、手写数字识别。*

- **Binary Cross Entropy (二元交叉熵 / Log Loss)**
  - **搭配**：输出层通常配合 **Sigmoid**。
  - **应用场景**：**二分类**任务（是/否，真/假）。
  - **变体**：Focal Loss（RetinaNet 中提出），用于解决正负样本极端不平衡的目标检测问题。
- **Categorical Cross Entropy (多类交叉熵)**
  - **搭配**：输出层通常配合 **Softmax**。
  - **应用场景**：**多分类**任务（ImageNet 1000类分类，手写数字 0-9）。它是衡量两个概率分布（预测分布 vs 真实分布）差异的标准方法。

#### 3. 特殊任务 (相似度/嵌入)

- **Hinge Loss (铰链损失)**
  - **应用场景**：**SVM (支持向量机)**。它致力于寻找最大间隔分类面。
- **Triplet Loss (三元组损失)**
  - **应用场景**：**人脸识别 (Face Recognition)**、图像检索。
  - **目的**：让“同一个人”的照片距离更近，让“不同人”的照片距离更远。

## 为什么使用 Transformer

### 卷积网络:

仅考虑局部信息
忽视输入的全局关系
难以处理序列

### 循环神经网络(现已不再使用)

无法处理长序列(易遗忘)
参数众多

### Transformer:

高效建立处理序列(如句子和视频)的长距离依赖
设计用于翻译任务



## **Transformer** 

### 1. 核心理念：抛弃循环，拥抱并行

在 Transformer 出现之前，处理序列数据（如文本、语音）的主流是 **RNN (循环神经网络)** 和 **LSTM**。

- **RNN 的做法**：像人阅读一样，从左到右，读完“我”才能读“爱”，读完“爱”才能读“你”。
  - *缺点*：无法并行计算（慢），且读到长句子末尾时容易忘记开头（长距离依赖问题）。
- **Transformer 的做法**：**一眼看完整个句子**。它不再按顺序处理，而是把整句话同时输入网络，利用**注意力机制**瞬间捕捉所有词之间的关系。

------

### 2. 核心组件：自注意力机制 (Self-Attention) —— 灵魂所在

这是 Transformer 最天才的设计。它的目的是计算：**在一句话中，单词 A 和单词 B 到底有多大关系？**

为了实现这一点，Transformer 把每个单词（输入向量）拆分成了三个向量：

- **Q (Query)**：查询向量 —— “我在找什么？”
- **K (Key)**：键向量 —— “我有什么特征？”
- **V (Value)**：值向量 —— “我的实际内容是什么？”

#### **通俗比喻：图书馆查资料**

1. **Query (Q)**：你手里拿着一张书单（查询需求）。
2. **Key (K)**：图书馆里每本书脊上的标签（索引特征）。
3. **计算注意力 (Attention Score)**：你拿着手中的书单 ($Q$) 去和书架上每本书的标签 ($K$) 进行比对（点积运算）。
   - 如果匹配度高，分数就高（说明这本书很重要）。
   - 如果匹配度低，分数就低（说明这本书不相关）。
4. **Softmax**：把这些分数归一化，变成概率（权重）。比如：书 A 相关性 0.8，书 B 相关性 0.2。
5. **Value (V)**：最后，根据这个权重，把书的内容 ($V$) 取出来并加权求和。

数学公式：


$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
通过这个过程，单词“银行”在读到上下文是“河流”时，会更多地关注“岸边”；而在上下文是“金钱”时，会更多地关注“存取”。

------

### 3. 多头注意力 (Multi-Head Attention)

如果只用一组 $Q, K, V$，可能只能捕捉到一种关系。为了更强，Transformer 搞了“分身术”。

- **原理**：把 $Q, K, V$ 切分成多个头（比如 8 个头）。
- **作用**：让模型从**不同的角度**去理解句子。
  - 头 1 可能关注语法结构（主谓宾）。
  - 头 2 可能关注指代关系（“它”指代谁）。
  - 头 3 可能关注情感色彩。
- 最后把这 8 个头的结果拼起来，就得到了一个全方位的理解。

------

### 4. 位置编码 (Positional Encoding)

因为 Transformer 是一次性并行输入整句话，它本身是**没有“顺序”概念**的。对于 Transformer 来说，“我爱你”和“你爱我”如果不加处理，看起来是一样的。

- **解决方案**：人为地给每个词加上一个“位置标记”。
- **做法**：在输入向量中加上一个代表位置的向量（通常用正弦/余弦函数生成）。这样模型就知道哪个词在前，哪个词在后了。

------

### 5. 整体架构：Encoder-Decoder (编码器-解码器)

Transformer 的本体由两大部分堆叠而成（通常堆 6 层）：

#### **A. 编码器 (Encoder) —— 负责“理解”**

- **输入**：源句子（比如英文）。
- **流程**：输入 $\rightarrow$ 位置编码 $\rightarrow$ 多头自注意力 $\rightarrow$ 残差连接 & 归一化 $\rightarrow$ 前馈网络 (Feed Forward) $\rightarrow$ 输出。
- **产出**：一个包含了对整个句子深刻理解的上下文矩阵（Memory）。
- *注：现在的 BERT、Vision Transformer (ViT) 主要就是由 Encoder 组成的。*

#### **B. 解码器 (Decoder) —— 负责“生成”**

- **输入**：已经翻译出来的词（比如中文）。
- **流程**：它有两个注意力层。
  1. **Masked Self-Attention**：看自己已经翻译出的内容（但在训练时要遮住未来的词，不能偷看答案）。
  2. **Cross-Attention (交叉注意力)**：**这是关键**。它的 $Q$ 来自解码器（我在翻译什么？），但它的 $K$ 和 $V$ 来自**编码器**（源句子的信息）。这意味着解码器每生成一个字，都要回头去查阅编码器提供的“源句字典”。
- *注：现在的 GPT 系列主要就是由 Decoder 组成的（只做生成，不需要看源文）。*

------

### 6. 在计算机视觉中的应用 (Vision Transformer, ViT)

既然 Transformer 处理的是序列，怎么处理 2D 图片呢？

- **分块 (Patching)**：把一张图片（比如 $224 \times 224$）切成很多个小方块（比如 $16 \times 16$ 的 Patch）。
- **拉平 (Flatten)**：把每个小方块拉成一个向量。
- **序列化**：现在，这堆小方块向量就相当于句子中的“单词”。
- **输入**：把这些“图片单词”加上位置编码，喂给 Transformer Encoder。
- **结果**：ViT 证明了，只要数据量够大，Transformer 在图像识别上也能超过 CNN，因为它拥有**全局感受野**（第一层就能看到整张图，而 CNN 需要堆很多层才能看到全局）。

### 总结

Transformer 的成功秘诀在于：

1. **并行计算**：训练速度极快，能吃海量数据。
2. **全局注意力**：不受距离限制，能瞬间捕捉长距离依赖。
3. **通用性**：不管是文本、图像还是音频，只要能变成序列，它都能处理。
